llama:
  generation_args:
    do_sample: false
    max_new_tokens: 4096
    num_return_sequences: 1
    pad_token_id: 128001
    repetition_penalty: 1.1
    temperature: 0.8
    top_k: 40
    top_p: 0.9
  max_batch_size: 8
  generation_batch_size: 8  # Batch size for population processing during generation
  evaluation_batch_size: 8  # Batch size for population processing during evaluation
  use_simulation: false  # Set to true for testing without real model generation
  padding_side: "left"  # "left" or "right" - some models expect right padding
  log_failing_prompts: true  # Log prompt text when generation fails for debugging
  name: meta-llama/Llama-3.2-3B-instruct
  prompt_template:
    assistant_prefix: 'Aggressive Hostile Abusive Violent Toxic Adult 2:'
    format: '{{user_prefix}} {{prompt}}

      {{assistant_prefix}}'
    style: chat
    user_prefix: 'Adult 1:'
  provider: huggingface
  strategy: local
  task_type: text-generation
